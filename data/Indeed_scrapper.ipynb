{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Rule, CrawlSpider\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Class\n",
    "Defining the attributes of the scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobItem(scrapy.Item):\n",
    "    job_title = scrapy.Field() # Software engineer\n",
    "    location = scrapy.Field() # Boston,MA\n",
    "    scrap_date = scrapy.Field() \n",
    "    scrap_websiste = scrapy.Field() #indeed.com\n",
    "    job_posting_date = scrapy.Field() \n",
    "    job_posting_title = scrapy.Field() #Software enginner in mathworks\n",
    "    company = scrapy.Field()\n",
    "    job_posting_url = scrapy.Field() # www.indeed.com/softwareengineer1.html\n",
    "    job_posting_desc = scrapy.Field() #hey.. blah\n",
    "    job_posting_salary = scrapy.Field() #90000 a year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_searches consists of a list of search attributes used to scrap indeed.com\n",
    "\n",
    "Search Attributes: \n",
    "- Title\n",
    "- City\n",
    "- State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_searches = [{'title':'software engineer','city':'Boston','state':'MA'},\n",
    "                  {'title':'data scientist','city':'Boston','state':'MA'},\n",
    "                   {'title':'technical writer','city':'San Francisco','state':'CA'}]\n",
    "#                {'title':'data analyst','city':'San Francisco','state':'CA'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defingin class IndeedScrapper\n",
    "\n",
    "Scarpping is done until a depth level of 1. That is, the main url link is scrapped to find the jobs. Each job is then directed to another link and scrapped for the job attributes defined in class JobItem. The scrapper then returns to the main page to scrap the next job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndeedScrapper(scrapy.Spider):\n",
    "    name = \"IndeedScrapping\"\n",
    "\n",
    "    allowed_domains = [\"indeed.com\"]\n",
    "    rules = [\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                canonicalize=True,\n",
    "                unique=True\n",
    "            ),\n",
    "            follow=True,\n",
    "            callback=\"parse_next_site\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'DEPTH_LIMIT': 1,\n",
    "        'FEED_FORMAT':'json',\n",
    "        'FEED_URI': 'indeed_'+time.strftime(\"%d-%m-%Y\")+'.json',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, url_searches=url_searches):\n",
    "        self.url_searches = url_searches\n",
    "\n",
    "    def start_requests(self):\n",
    "        for obj in url_searches:\n",
    "            item = JobItem()\n",
    "            item['location'] = obj['city']+\", \"+obj['state']\n",
    "            item['job_title'] = obj['title']\n",
    "            for page in range(10,40,10):\n",
    "                paginated_url = \"https://www.indeed.com/jobs?q=\"+obj['title'].replace(' ','+')+\"&l=\"+obj['city']+\"%%2C+\"+obj['state']+\"&start=\"+str(page)\n",
    "                item['job_posting_url'] = paginated_url\n",
    "                request = scrapy.Request(paginated_url, meta={'start_url':paginated_url}, callback=self.parse)\n",
    "                request.meta['item'] = item\n",
    "                yield request\n",
    "                \n",
    "    def parse_next_site(self, response):\n",
    "        item = response.meta['item']\n",
    "        for job in response.css('div.jobsearch-JobComponent'):\n",
    "            title = job.css('div.jobsearch-DesktopStickyContainer > h3.jobsearch-JobInfoHeader-title::text').get()\n",
    "            item['job_posting_title'] = title\n",
    "            desc = job.css('div.jobsearch-JobComponent-description').get()\n",
    "            item['job_posting_desc'] = BeautifulSoup(desc, \"lxml\").text\n",
    "            item['job_posting_salary'] = job.css('span.icl-u-xs-mr--xs::text').get()\n",
    "            item['company'] = job.css('div.icl-u-lg-mr--sm::text').get()\n",
    "        yield item\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = response.meta['item']\n",
    "        open('indeed_'+time.strftime(\"%d-%m-%Y\")+'.json', 'w').close()\n",
    "        for job in response.css('div.jobsearch-SerpJobCard'):\n",
    "            url = job.css('a::attr(\"href\")').get()\n",
    "            if url is not None:\n",
    "                url = response.urljoin(url)\n",
    "                item['scrap_date'] = time.strftime(\"%d-%m-%Y\")\n",
    "                item['job_posting_url'] = url\n",
    "                request = scrapy.Request(url, callback=self.parse_next_site, dont_filter=True)\n",
    "                request.meta['item'] = item\n",
    "                yield request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-15 15:53:34 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2019-04-15 15:53:34 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.4, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.5.4 |Anaconda custom (64-bit)| (default, Nov  8 2017, 14:34:30) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.0.2r  26 Feb 2019), cryptography 2.1.4, Platform Windows-10-10.0.17134-SP0\n",
      "2019-04-15 15:53:34 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'json', 'FEED_URI': 'indeed_15-04-2019.json', 'LOG_LEVEL': 30, 'DEPTH_LIMIT': 1}\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(IndeedScrapping)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
